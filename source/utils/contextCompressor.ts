import {getOpenAiConfig, getCustomSystemPrompt} from './apiConfig.js';
import {getSystemPrompt} from '../api/systemPrompt.js';
import type {ChatMessage} from '../api/types.js';
import {createStreamingChatCompletion} from '../api/chat.js';
import {createStreamingResponse} from '../api/responses.js';
import {createStreamingGeminiCompletion} from '../api/gemini.js';
import {createStreamingAnthropicCompletion} from '../api/anthropic.js';

export interface CompressionResult {
	summary: string;
	usage: {
		prompt_tokens: number;
		completion_tokens: number;
		total_tokens: number;
	};
	preservedMessages?: ChatMessage[];
}

/**
 * Compression request prompt - asks AI to create a detailed, structured summary
 * that preserves critical information for task continuity
 */
const COMPRESSION_PROMPT = `You are compressing a conversation history to save context space while preserving all critical information. Create a comprehensive summary following this structure:

## ğŸ“‹ Current Task & Goals
- What is the main task or project being worked on?
- What are the specific objectives and desired outcomes?
- What is the current progress status?

## ğŸ”§ Technical Context
- Key technologies, frameworks, libraries, and tools being used
- Important file paths, function names, and code locations mentioned
- Architecture decisions and design patterns chosen
- Configuration settings and environment details

## ğŸ’¡ Key Decisions & Approaches
- Important decisions made and their rationale
- Chosen approaches and methodologies
- Solutions to problems encountered
- Best practices or patterns agreed upon

## âœ… Completed Work
- What has been successfully implemented or resolved?
- Important code changes, fixes, or features added
- Test results or validation performed

## ğŸš§ Pending & In-Progress Work
- What tasks are currently unfinished?
- Known issues or blockers that need addressing
- Next steps planned or discussed
- Open questions or areas needing clarification

## ğŸ”‘ Critical Information
- Important data, values, IDs, or credentials referenced (sanitized)
- Error messages, warnings, or diagnostic information
- User preferences, requirements, or constraints
- Any other context essential for seamless continuation

**Guidelines:**
- Be specific with names, paths, and technical details
- Preserve exact terminology and technical vocabulary
- Include enough detail to continue work without confusion
- Use code snippets or examples where helpful
- Prioritize actionable information over general descriptions`;

/**
 * æ‰¾åˆ°éœ€è¦ä¿ç•™çš„æ¶ˆæ¯ï¼ˆæœ€è¿‘çš„å·¥å…·è°ƒç”¨é“¾ï¼‰
 *
 * ä¿ç•™ç­–ç•¥ï¼š
 * - å¦‚æœæœ€åæœ‰æœªå®Œæˆçš„å·¥å…·è°ƒç”¨ï¼ˆassistant with tool_calls æˆ– toolï¼‰ï¼Œä¿ç•™è¿™ä¸ªé“¾
 * - å¦‚æœæœ€åæ˜¯æ™®é€š assistant æˆ– userï¼Œä¸éœ€è¦ä¿ç•™ï¼ˆå‹ç¼©å…¨éƒ¨ï¼‰
 *
 * æ³¨æ„ï¼šä¸ä¿ç•™ user æ¶ˆæ¯ï¼Œå› ä¸ºï¼š
 * 1. å‹ç¼©æ‘˜è¦å·²åŒ…å«å†å²ä¸Šä¸‹æ–‡
 * 2. ä¸‹ä¸€è½®å¯¹è¯ä¼šæœ‰æ–°çš„ user æ¶ˆæ¯
 *
 * @returns ä¿ç•™æ¶ˆæ¯çš„èµ·å§‹ç´¢å¼•ï¼Œå¦‚æœå…¨éƒ¨å‹ç¼©åˆ™è¿”å› messages.length
 */
function findPreserveStartIndex(messages: ChatMessage[]): number {
	if (messages.length === 0) {
		return 0;
	}

	const lastMsg = messages[messages.length - 1];

	// Case 1: æœ€åæ˜¯ tool æ¶ˆæ¯ â†’ ä¿ç•™ assistant(tool_calls) â†’ tool
	if (lastMsg?.role === 'tool') {
		// å‘å‰æ‰¾å¯¹åº”çš„ assistant with tool_calls
		for (let i = messages.length - 2; i >= 0; i--) {
			const msg = messages[i];
			if (msg?.role === 'assistant' && msg.tool_calls && msg.tool_calls.length > 0) {
				// æ‰¾åˆ°äº†ï¼Œä»è¿™ä¸ª assistant å¼€å§‹ä¿ç•™
				return i;
			}
		}
		// å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”çš„ assistantï¼Œä¿ç•™æœ€åçš„ toolï¼ˆè™½ç„¶ä¸å¤ªå¯èƒ½ï¼‰
		return messages.length - 1;
	}

	// Case 2: æœ€åæ˜¯ assistant with tool_calls â†’ ä¿ç•™ assistant(tool_calls)
	if (lastMsg?.role === 'assistant' && lastMsg.tool_calls && lastMsg.tool_calls.length > 0) {
		// ä¿ç•™è¿™ä¸ªå¾…å¤„ç†çš„ tool_calls
		return messages.length - 1;
	}

	// Case 3: æœ€åæ˜¯æ™®é€š assistant æˆ– user â†’ å…¨éƒ¨å‹ç¼©
	// å› ä¸ºæ²¡æœ‰æœªå®Œæˆçš„å·¥å…·è°ƒç”¨é“¾
	return messages.length;
}

/**
 * Prepare messages for compression by adding system prompt and compression request
 * Note: Only filters out system messages and tool messages, preserving user and assistant messages
 */
function prepareMessagesForCompression(
	conversationMessages: ChatMessage[],
	customSystemPrompt: string | null,
): ChatMessage[] {
	const messages: ChatMessage[] = [];

	// Add system prompt (handled by API modules)
	if (customSystemPrompt) {
		// If custom system prompt exists: custom as system, default as first user message
		messages.push({role: 'system', content: customSystemPrompt});
		messages.push({role: 'user', content: getSystemPrompt()});
	} else {
		// No custom system prompt: default as system
		messages.push({role: 'system', content: getSystemPrompt()});
	}

	// Add all conversation history for compression
	// Filter out system messages (already added above) and tool messages (only needed for API, not for summary)
	for (const msg of conversationMessages) {
		if (msg.role !== 'system' && msg.role !== 'tool') {
			// Only include user and assistant messages for compression
			messages.push({
				role: msg.role,
				content: msg.content,
			});
		}
	}

	// Add compression request as final user message
	messages.push({
		role: 'user',
		content: COMPRESSION_PROMPT,
	});

	return messages;
}

/**
 * Compress context using OpenAI Chat Completions API (reuses chat.ts)
 */
async function compressWithChatCompletions(
	modelName: string,
	conversationMessages: ChatMessage[],
	customSystemPrompt: string | null,
): Promise<CompressionResult> {
	const messages = prepareMessagesForCompression(
		conversationMessages,
		customSystemPrompt,
	);

	let summary = '';
	let usage = {
		prompt_tokens: 0,
		completion_tokens: 0,
		total_tokens: 0,
	};

	// Use the existing streaming API from chat.ts (includes proxy support)
	for await (const chunk of createStreamingChatCompletion({
		model: modelName,
		messages,
		stream: true,
	})) {
		// Collect content
		if (chunk.type === 'content' && chunk.content) {
			summary += chunk.content;
		}

		// Collect usage info
		if (chunk.type === 'usage' && chunk.usage) {
			usage = {
				prompt_tokens: chunk.usage.prompt_tokens || 0,
				completion_tokens: chunk.usage.completion_tokens || 0,
				total_tokens: chunk.usage.total_tokens || 0,
			};
		}
	}

	if (!summary) {
		throw new Error('Failed to generate summary from compact model');
	}

	return {summary, usage};
}

/**
 * Compress context using OpenAI Responses API (reuses responses.ts)
 */
async function compressWithResponses(
	modelName: string,
	conversationMessages: ChatMessage[],
	customSystemPrompt: string | null,
): Promise<CompressionResult> {
	const messages = prepareMessagesForCompression(
		conversationMessages,
		customSystemPrompt,
	);

	let summary = '';
	let usage = {
		prompt_tokens: 0,
		completion_tokens: 0,
		total_tokens: 0,
	};

	// Use the existing streaming API from responses.ts (includes proxy support)
	for await (const chunk of createStreamingResponse({
		model: modelName,
		messages,
		stream: true,
	})) {
		// Collect content
		if (chunk.type === 'content' && chunk.content) {
			summary += chunk.content;
		}

		// Collect usage info
		if (chunk.type === 'usage' && chunk.usage) {
			usage = {
				prompt_tokens: chunk.usage.prompt_tokens || 0,
				completion_tokens: chunk.usage.completion_tokens || 0,
				total_tokens: chunk.usage.total_tokens || 0,
			};
		}
	}

	if (!summary) {
		throw new Error(
			'Failed to generate summary from compact model (Responses API)',
		);
	}

	return {summary, usage};
}

/**
 * Compress context using Gemini API (reuses gemini.ts)
 */
async function compressWithGemini(
	modelName: string,
	conversationMessages: ChatMessage[],
	customSystemPrompt: string | null,
): Promise<CompressionResult> {
	const messages = prepareMessagesForCompression(
		conversationMessages,
		customSystemPrompt,
	);

	let summary = '';
	let usage = {
		prompt_tokens: 0,
		completion_tokens: 0,
		total_tokens: 0,
	};

	// Use the existing streaming API from gemini.ts (includes proxy support)
	for await (const chunk of createStreamingGeminiCompletion({
		model: modelName,
		messages,
	})) {
		// Collect content
		if (chunk.type === 'content' && chunk.content) {
			summary += chunk.content;
		}

		// Collect usage info
		if (chunk.type === 'usage' && chunk.usage) {
			usage = {
				prompt_tokens: chunk.usage.prompt_tokens || 0,
				completion_tokens: chunk.usage.completion_tokens || 0,
				total_tokens: chunk.usage.total_tokens || 0,
			};
		}
	}

	if (!summary) {
		throw new Error('Failed to generate summary from Gemini model');
	}

	return {summary, usage};
}

/**
 * Compress context using Anthropic API (reuses anthropic.ts)
 */
async function compressWithAnthropic(
	modelName: string,
	conversationMessages: ChatMessage[],
	customSystemPrompt: string | null,
): Promise<CompressionResult> {
	const messages = prepareMessagesForCompression(
		conversationMessages,
		customSystemPrompt,
	);

	let summary = '';
	let usage = {
		prompt_tokens: 0,
		completion_tokens: 0,
		total_tokens: 0,
	};

	// Use the existing streaming API from anthropic.ts (includes proxy support)
	for await (const chunk of createStreamingAnthropicCompletion({
		model: modelName,
		messages,
		max_tokens: 4096,
	})) {
		// Collect content
		if (chunk.type === 'content' && chunk.content) {
			summary += chunk.content;
		}

		// Collect usage info
		if (chunk.type === 'usage' && chunk.usage) {
			usage = {
				prompt_tokens: chunk.usage.prompt_tokens || 0,
				completion_tokens: chunk.usage.completion_tokens || 0,
				total_tokens: chunk.usage.total_tokens || 0,
			};
		}
	}

	if (!summary) {
		throw new Error('Failed to generate summary from Anthropic model');
	}

	return {summary, usage};
}

/**
 * Compress conversation history using the compact model
 * @param messages - Array of messages to compress
 * @returns Compressed summary and token usage information, or null if compression should be skipped
 */
export async function compressContext(
	messages: ChatMessage[],
): Promise<CompressionResult | null> {
	const config = getOpenAiConfig();

	// Check if compact model is configured
	if (!config.compactModel || !config.compactModel.modelName) {
		throw new Error(
			'Compact model not configured. Please configure it in API & Model Settings.',
		);
	}

	if (messages.length === 0) {
		console.warn('No messages to compress');
		return null;
	}

	const modelName = config.compactModel.modelName;
	const requestMethod = config.requestMethod;

	// Get custom system prompt if configured
	const customSystemPrompt = getCustomSystemPrompt();

	// æ‰¾åˆ°éœ€è¦ä¿ç•™çš„æ¶ˆæ¯èµ·å§‹ä½ç½®
	const preserveStartIndex = findPreserveStartIndex(messages);

	// å¦‚æœ preserveStartIndex ä¸º 0ï¼Œè¯´æ˜æ‰€æœ‰æ¶ˆæ¯éƒ½éœ€è¦ä¿ç•™ï¼ˆæ²¡æœ‰å†å²å¯å‹ç¼©ï¼‰
	// ä¾‹å¦‚ï¼šæ•´ä¸ªå¯¹è¯åªæœ‰ä¸€æ¡ userâ†’assistant(tool_calls)ï¼Œæ— æ³•å‹ç¼©
	if (preserveStartIndex === 0) {
		console.warn('Cannot compress: all messages need to be preserved (no history)');
		return null;
	}

	// åˆ†ç¦»å¾…å‹ç¼©å’Œå¾…ä¿ç•™çš„æ¶ˆæ¯
	const messagesToCompress = messages.slice(0, preserveStartIndex);
	const preservedMessages = messages.slice(preserveStartIndex);

	try {
		// Choose compression method based on request method
		// All methods now reuse existing API modules which include proxy support
		let result: CompressionResult;

		switch (requestMethod) {
			case 'gemini':
				result = await compressWithGemini(
					modelName,
					messagesToCompress,
					customSystemPrompt || null,
				);
				break;

			case 'anthropic':
				result = await compressWithAnthropic(
					modelName,
					messagesToCompress,
					customSystemPrompt || null,
				);
				break;

			case 'responses':
				// OpenAI Responses API
				result = await compressWithResponses(
					modelName,
					messagesToCompress,
					customSystemPrompt || null,
				);
				break;

			case 'chat':
			default:
				// OpenAI Chat Completions API
				result = await compressWithChatCompletions(
					modelName,
					messagesToCompress,
					customSystemPrompt || null,
				);
				break;
		}

		// æ·»åŠ ä¿ç•™çš„æ¶ˆæ¯åˆ°ç»“æœä¸­
		if (preservedMessages.length > 0) {
			result.preservedMessages = preservedMessages;
		}

		return result;
	} catch (error) {
		if (error instanceof Error) {
			throw new Error(`Context compression failed: ${error.message}`);
		}
		throw new Error('Unknown error occurred during context compression');
	}
}
