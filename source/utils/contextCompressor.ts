import {getOpenAiConfig, getCustomSystemPrompt} from './apiConfig.js';
import {getSystemPrompt} from '../api/systemPrompt.js';
import type {ChatMessage} from '../api/types.js';
import {createStreamingChatCompletion} from '../api/chat.js';
import {createStreamingResponse} from '../api/responses.js';
import {createStreamingGeminiCompletion} from '../api/gemini.js';
import {createStreamingAnthropicCompletion} from '../api/anthropic.js';

export interface CompressionResult {
	summary: string;
	usage: {
		prompt_tokens: number;
		completion_tokens: number;
		total_tokens: number;
	};
	preservedMessages?: ChatMessage[]; // ä¿ç•™çš„å50%åŸå§‹æ¶ˆæ¯
}

/**
 * Compression request prompt - asks AI to create a detailed, structured summary
 * that preserves critical information for task continuity
 */
const COMPRESSION_PROMPT = `You are compressing a conversation history to save context space while preserving all critical information. Create a comprehensive summary following this structure:

## ğŸ“‹ Current Task & Goals
- What is the main task or project being worked on?
- What are the specific objectives and desired outcomes?
- What is the current progress status?

## ğŸ”§ Technical Context
- Key technologies, frameworks, libraries, and tools being used
- Important file paths, function names, and code locations mentioned
- Architecture decisions and design patterns chosen
- Configuration settings and environment details

## ğŸ’¡ Key Decisions & Approaches
- Important decisions made and their rationale
- Chosen approaches and methodologies
- Solutions to problems encountered
- Best practices or patterns agreed upon

## âœ… Completed Work
- What has been successfully implemented or resolved?
- Important code changes, fixes, or features added
- Test results or validation performed

## ğŸš§ Pending & In-Progress Work
- What tasks are currently unfinished?
- Known issues or blockers that need addressing
- Next steps planned or discussed
- Open questions or areas needing clarification

## ğŸ”‘ Critical Information
- Important data, values, IDs, or credentials referenced (sanitized)
- Error messages, warnings, or diagnostic information
- User preferences, requirements, or constraints
- Any other context essential for seamless continuation

**Guidelines:**
- Be specific with names, paths, and technical details
- Preserve exact terminology and technical vocabulary
- Include enough detail to continue work without confusion
- Use code snippets or examples where helpful
- Prioritize actionable information over general descriptions`;

/**
 * æ™ºèƒ½æ‰¾åˆ°å¯¹è¯çš„50%åˆ†å‰²ç‚¹ï¼Œç¡®ä¿ä¸ç ´åå¯¹è¯ç»“æ„
 *
 * ä¸¥æ ¼çš„æ¶ˆæ¯ç»“æ„è¦æ±‚ï¼š
 * 1. user â†’ assistant (å¯èƒ½åŒ…å« tool_calls)
 * 2. å¦‚æœ assistant æœ‰ tool_calls â†’ å¿…é¡»è·Ÿéšå¯¹åº”çš„ tool ç»“æœæ¶ˆæ¯
 * 3. tool ç»“æœæ¶ˆæ¯ â†’ å¿…é¡»è·Ÿéš assistant çš„å“åº”
 *
 * å®‰å…¨åˆ†å‰²ç‚¹ï¼šuser æ¶ˆæ¯ä¹‹å‰ï¼ˆç¡®ä¿å‰ä¸€è½®å¯¹è¯å®Œå…¨ç»“æŸï¼‰
 */
function findSplitPoint(messages: ChatMessage[]): number {
	if (messages.length <= 2) {
		return 0; // æ¶ˆæ¯å¤ªå°‘ï¼Œä¸åˆ†å‰²
	}

	// è®¡ç®—ä¸­ç‚¹ä½ç½®ï¼ˆæŒ‰æ¶ˆæ¯æ•°é‡ï¼‰
	const midPoint = Math.floor(messages.length / 2);

	// ä»ä¸­ç‚¹å‘åæŸ¥æ‰¾ï¼Œæ‰¾åˆ°ä¸‹ä¸€ä¸ª user æ¶ˆæ¯
	// åœ¨ user æ¶ˆæ¯å‰åˆ†å‰²ï¼Œç¡®ä¿å‰ä¸€è½®å¯¹è¯å®Œå…¨ç»“æŸ
	for (let i = midPoint; i < messages.length; i++) {
		const msg = messages[i];
		if (!msg) continue;

		if (msg.role === 'user') {
			// éªŒè¯ï¼šç¡®ä¿å‰ä¸€æ¡æ¶ˆæ¯ä¸æ˜¯å¾…å¤„ç†çš„ tool_calls
			if (i > 0) {
				const prevMsg = messages[i - 1];
				if (prevMsg && prevMsg.role === 'assistant' && prevMsg.tool_calls && prevMsg.tool_calls.length > 0) {
					// å‰ä¸€æ¡æ˜¯å¸¦ tool_calls çš„ assistantï¼Œéœ€è¦ç»§ç»­æŸ¥æ‰¾
					continue;
				}
			}
			// å®‰å…¨çš„åˆ†å‰²ç‚¹ï¼šåœ¨ user æ¶ˆæ¯å‰åˆ†å‰²
			return i;
		}
	}

	// å¦‚æœå‘åæ‰¾ä¸åˆ° user æ¶ˆæ¯ï¼Œå‘å‰æŸ¥æ‰¾
	for (let i = midPoint - 1; i > 0; i--) {
		const msg = messages[i];
		if (!msg) continue;

		if (msg.role === 'user') {
			// åŒæ ·éªŒè¯å‰ä¸€æ¡æ¶ˆæ¯
			if (i > 0) {
				const prevMsg = messages[i - 1];
				if (prevMsg && prevMsg.role === 'assistant' && prevMsg.tool_calls && prevMsg.tool_calls.length > 0) {
					// å‰ä¸€æ¡æ˜¯å¸¦ tool_calls çš„ assistantï¼Œç»§ç»­å‘å‰æŸ¥æ‰¾
					continue;
				}
			}
			return i;
		}
	}

	// å®åœ¨æ‰¾ä¸åˆ°å®‰å…¨çš„åˆ†å‰²ç‚¹ï¼Œæ£€æŸ¥ä¸­ç‚¹æ˜¯å¦å®‰å…¨
	// å¦‚æœä¸­ç‚¹ä¸å®‰å…¨ï¼ˆåœ¨å·¥å…·è°ƒç”¨é“¾ä¸­é—´ï¼‰ï¼Œå‘å‰æ‰¾åˆ°ç¬¬ä¸€ä¸ª user æ¶ˆæ¯
	for (let i = 1; i < messages.length; i++) {
		const msg = messages[i];
		if (msg && msg.role === 'user') {
			const prevMsg = messages[i - 1];
			if (!prevMsg || prevMsg.role !== 'assistant' || !prevMsg.tool_calls || prevMsg.tool_calls.length === 0) {
				return i;
			}
		}
	}

	// æç«¯æƒ…å†µï¼šæ•´ä¸ªå¯¹è¯éƒ½æ˜¯è¿ç»­çš„å·¥å…·è°ƒç”¨é“¾ï¼Œä¸åˆ†å‰²
	return 0;
}

/**
 * Prepare messages for compression by adding system prompt and compression request
 */
function prepareMessagesForCompression(
	conversationMessages: ChatMessage[],
	customSystemPrompt: string | null,
): ChatMessage[] {
	const messages: ChatMessage[] = [];

	// Add system prompt (handled by API modules)
	if (customSystemPrompt) {
		// If custom system prompt exists: custom as system, default as first user message
		messages.push({role: 'system', content: customSystemPrompt});
		messages.push({role: 'user', content: getSystemPrompt()});
	} else {
		// No custom system prompt: default as system
		messages.push({role: 'system', content: getSystemPrompt()});
	}

	// Add all conversation history (exclude system and tool messages)
	for (const msg of conversationMessages) {
		if (msg.role !== 'system' && msg.role !== 'tool') {
			messages.push({
				role: msg.role,
				content: msg.content,
			});
		}
	}

	// Add compression request as final user message
	messages.push({
		role: 'user',
		content: COMPRESSION_PROMPT,
	});

	return messages;
}

/**
 * Compress context using OpenAI Chat Completions API (reuses chat.ts)
 */
async function compressWithChatCompletions(
	modelName: string,
	conversationMessages: ChatMessage[],
	customSystemPrompt: string | null,
): Promise<CompressionResult> {
	const messages = prepareMessagesForCompression(
		conversationMessages,
		customSystemPrompt,
	);

	let summary = '';
	let usage = {
		prompt_tokens: 0,
		completion_tokens: 0,
		total_tokens: 0,
	};

	// Use the existing streaming API from chat.ts (includes proxy support)
	for await (const chunk of createStreamingChatCompletion({
		model: modelName,
		messages,
		stream: true,
	})) {
		// Collect content
		if (chunk.type === 'content' && chunk.content) {
			summary += chunk.content;
		}

		// Collect usage info
		if (chunk.type === 'usage' && chunk.usage) {
			usage = {
				prompt_tokens: chunk.usage.prompt_tokens || 0,
				completion_tokens: chunk.usage.completion_tokens || 0,
				total_tokens: chunk.usage.total_tokens || 0,
			};
		}
	}

	if (!summary) {
		throw new Error('Failed to generate summary from compact model');
	}

	return {summary, usage};
}

/**
 * Compress context using OpenAI Responses API (reuses responses.ts)
 */
async function compressWithResponses(
	modelName: string,
	conversationMessages: ChatMessage[],
	customSystemPrompt: string | null,
): Promise<CompressionResult> {
	const messages = prepareMessagesForCompression(
		conversationMessages,
		customSystemPrompt,
	);

	let summary = '';
	let usage = {
		prompt_tokens: 0,
		completion_tokens: 0,
		total_tokens: 0,
	};

	// Use the existing streaming API from responses.ts (includes proxy support)
	for await (const chunk of createStreamingResponse({
		model: modelName,
		messages,
		stream: true,
	})) {
		// Collect content
		if (chunk.type === 'content' && chunk.content) {
			summary += chunk.content;
		}

		// Collect usage info
		if (chunk.type === 'usage' && chunk.usage) {
			usage = {
				prompt_tokens: chunk.usage.prompt_tokens || 0,
				completion_tokens: chunk.usage.completion_tokens || 0,
				total_tokens: chunk.usage.total_tokens || 0,
			};
		}
	}

	if (!summary) {
		throw new Error(
			'Failed to generate summary from compact model (Responses API)',
		);
	}

	return {summary, usage};
}

/**
 * Compress context using Gemini API (reuses gemini.ts)
 */
async function compressWithGemini(
	modelName: string,
	conversationMessages: ChatMessage[],
	customSystemPrompt: string | null,
): Promise<CompressionResult> {
	const messages = prepareMessagesForCompression(
		conversationMessages,
		customSystemPrompt,
	);

	let summary = '';
	let usage = {
		prompt_tokens: 0,
		completion_tokens: 0,
		total_tokens: 0,
	};

	// Use the existing streaming API from gemini.ts (includes proxy support)
	for await (const chunk of createStreamingGeminiCompletion({
		model: modelName,
		messages,
	})) {
		// Collect content
		if (chunk.type === 'content' && chunk.content) {
			summary += chunk.content;
		}

		// Collect usage info
		if (chunk.type === 'usage' && chunk.usage) {
			usage = {
				prompt_tokens: chunk.usage.prompt_tokens || 0,
				completion_tokens: chunk.usage.completion_tokens || 0,
				total_tokens: chunk.usage.total_tokens || 0,
			};
		}
	}

	if (!summary) {
		throw new Error('Failed to generate summary from Gemini model');
	}

	return {summary, usage};
}

/**
 * Compress context using Anthropic API (reuses anthropic.ts)
 */
async function compressWithAnthropic(
	modelName: string,
	conversationMessages: ChatMessage[],
	customSystemPrompt: string | null,
): Promise<CompressionResult> {
	const messages = prepareMessagesForCompression(
		conversationMessages,
		customSystemPrompt,
	);

	let summary = '';
	let usage = {
		prompt_tokens: 0,
		completion_tokens: 0,
		total_tokens: 0,
	};

	// Use the existing streaming API from anthropic.ts (includes proxy support)
	for await (const chunk of createStreamingAnthropicCompletion({
		model: modelName,
		messages,
		max_tokens: 4096,
	})) {
		// Collect content
		if (chunk.type === 'content' && chunk.content) {
			summary += chunk.content;
		}

		// Collect usage info
		if (chunk.type === 'usage' && chunk.usage) {
			usage = {
				prompt_tokens: chunk.usage.prompt_tokens || 0,
				completion_tokens: chunk.usage.completion_tokens || 0,
				total_tokens: chunk.usage.total_tokens || 0,
			};
		}
	}

	if (!summary) {
		throw new Error('Failed to generate summary from Anthropic model');
	}

	return {summary, usage};
}

/**
 * Compress conversation history using the compact model
 * @param messages - Array of messages to compress
 * @param partialCompression - If true, only compress first 50% and preserve last 50%
 * @returns Compressed summary and token usage information
 */
export async function compressContext(
	messages: ChatMessage[],
	partialCompression: boolean = true,
): Promise<CompressionResult> {
	const config = getOpenAiConfig();

	// Check if compact model is configured
	if (!config.compactModel || !config.compactModel.modelName) {
		throw new Error(
			'Compact model not configured. Please configure it in API & Model Settings.',
		);
	}

	const modelName = config.compactModel.modelName;
	const requestMethod = config.requestMethod;

	// Get custom system prompt if configured
	const customSystemPrompt = getCustomSystemPrompt();

	let messagesToCompress = messages;
	let preservedMessages: ChatMessage[] | undefined;

	// å¦‚æœå¯ç”¨éƒ¨åˆ†å‹ç¼©ï¼Œåªå‹ç¼©å‰50%
	if (partialCompression && messages.length > 2) {
		const splitPoint = findSplitPoint(messages);

		if (splitPoint > 0) {
			// åˆ†å‰²æ¶ˆæ¯ï¼šå‰åŠéƒ¨åˆ†å‹ç¼©ï¼ŒååŠéƒ¨åˆ†ä¿ç•™
			messagesToCompress = messages.slice(0, splitPoint);
			preservedMessages = messages.slice(splitPoint);
		}
	}

	try {
		// Choose compression method based on request method
		// All methods now reuse existing API modules which include proxy support
		let result: CompressionResult;

		switch (requestMethod) {
			case 'gemini':
				result = await compressWithGemini(
					modelName,
					messagesToCompress,
					customSystemPrompt || null,
				);
				break;

			case 'anthropic':
				result = await compressWithAnthropic(
					modelName,
					messagesToCompress,
					customSystemPrompt || null,
				);
				break;

			case 'responses':
				// OpenAI Responses API
				result = await compressWithResponses(
					modelName,
					messagesToCompress,
					customSystemPrompt || null,
				);
				break;

			case 'chat':
			default:
				// OpenAI Chat Completions API
				result = await compressWithChatCompletions(
					modelName,
					messagesToCompress,
					customSystemPrompt || null,
				);
				break;
		}

		// æ·»åŠ ä¿ç•™çš„æ¶ˆæ¯åˆ°ç»“æœä¸­
		if (preservedMessages && preservedMessages.length > 0) {
			result.preservedMessages = preservedMessages;
		}

		return result;
	} catch (error) {
		if (error instanceof Error) {
			throw new Error(`Context compression failed: ${error.message}`);
		}
		throw new Error('Unknown error occurred during context compression');
	}
}
